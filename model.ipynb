{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPitnRG26y/kQQz8b0H55nl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyDaniel/PM2.5_prediction_regression/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7TeA9Q1qt95",
        "colab_type": "text"
      },
      "source": [
        "# **PREDICTING PM2.5 WITH LINEAR REGRESSION, TECHINIQUES INCLUDING FEATURE ENGINEERING, CROSS-VALIDATION, NORMALIZATION, AND ADAGRAD GRADIENT OPTIMIZATION.**\n",
        "\n",
        "Kaggle: https://www.kaggle.com/c/ml2020spring-hw1/overview\n",
        "\n",
        "Hanyu Song 03/19/2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYrq3fUsb0tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# import train data from google drive\n",
        "! gdown --id '1VR_MKDGwhexEThy4VEZudoN0zKgNRpys'\n",
        "train = pd.read_csv('./train.csv', encoding = 'big5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDLB4UZiHbt2",
        "colab_type": "text"
      },
      "source": [
        "## **FEATURE ENGINEERING FOR TRAINING DATASET**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ApB6JLyuIfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete unnecessary columns, fill in NR values, change to numpy form\n",
        "train = train.iloc[:, 3:]\n",
        "train[train == 'NR'] = 0\n",
        "train_data = train.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPPZzHpjHfS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now split train data into 12 (months) blocks in which each block contains 18 (features )*480 (24 hours * 20 days) info\n",
        "monthly_train = {}\n",
        "for month in range(12):\n",
        "  temp = np.empty((18, 480))\n",
        "  for day in range(20):\n",
        "    temp[:, day*24 : (day + 1)*24] = train_data[18 * (20*month + day) : 18 * (20 * month + day + 1), :]\n",
        "  monthly_train[month] = temp;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXXJAyfEQJSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will use previous 9 hours to data to predict the next, which is the 10th hour of pm2.5 level. \n",
        "# Therefore, out of the 480 hours we have for a given month, we have 471 sets of data (480 - 9) that can be used for training.\n",
        "# y shape: (12*471) * 1\n",
        "# x shape: (12*471) * (9*18)\n",
        "# weight shape: (9*18) * 1\n",
        "# y = x*w\n",
        "\n",
        "x = np.empty((12*471, 9*18), dtype = float)\n",
        "y = np.empty((12*471, 1), dtype = float)\n",
        "\n",
        "for month in range(12):\n",
        "  for day in range(20):\n",
        "    for hour in range(24):\n",
        "      if day == 0 and hour < 9:\n",
        "          continue\n",
        "      y[month * 471 + day * 24 + hour - 9, 0] = monthly_train[month][9, day * 24 + hour] #PM2.5 is on row 9\n",
        "\n",
        "for month in range(12):\n",
        "  for day in range(20):\n",
        "    for hour in range(24):\n",
        "      if day == 19 and hour > 14: \n",
        "        continue\n",
        "      x[month * 471 + day * 24 + hour, :] = monthly_train[month][:, day * 24 + hour : day * 24 + hour + 9].reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNP6ekTSHnRr",
        "colab_type": "text"
      },
      "source": [
        "## **NORMALIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEzkzU6lF1SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for each of the 18 features, compute their mean and std \n",
        "# then use newdata = (data - mean) /std to update x\n",
        "mean_x = np.mean(x, axis = 0) #18 * 9 \n",
        "std_x = np.std(x, axis = 0) #18 * 9 \n",
        "normed_x = np.empty(x.shape, dtype = float)\n",
        "for i in range(len(x)): #12 * 471\n",
        "    for j in range(len(x[0])): #18 * 9 \n",
        "        if std_x[j] != 0:\n",
        "            normed_x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knF_dm5ha088",
        "colab_type": "text"
      },
      "source": [
        "## **CROSS-VALIDATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6YkjojRaz2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is an illustration of how we would split dataset into 4:1 train and validate\n",
        "# x_train_set = x[: math.floor(len(normed_x) * 0.8), :]\n",
        "# y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
        "# x_validation = x[math.floor(len(normed_x) * 0.8): , :]\n",
        "# y_validation = y[math.floor(len(y) * 0.8): , :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nehvGmDK8d9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random as rand\n",
        "\n",
        "dim = 9 * 18 + 1 # 9 * 18 features with 1 more constant\n",
        "learning_rate = 100\n",
        "iter_time = 10000\n",
        "adagrad = np.zeros([dim, 1])\n",
        "eps = 0.0000000001\n",
        "sum_loss = 0\n",
        "k_fold = 5\n",
        "\n",
        "# Do 5-fold cross-validation on the normed_x and y\n",
        "for i in range(k_fold):\n",
        "  w = np.zeros((dim, 1))\n",
        "  # create training and validating data for each iteration\n",
        "  x_train_set = np.concatenate((normed_x[: math.floor(len(normed_x) * (0.2*i)), :], normed_x[math.floor(len(normed_x) * 0.2*(i + 1)) :, :]))\n",
        "  x_validate_set = normed_x[math.floor(len(normed_x) * (0.2*i)) : math.floor(len(normed_x) * 0.2* (i + 1))]\n",
        "  y_train_set = np.concatenate((y[:math.floor(len(y) * (0.2*i)), :], y[math.floor(len(y) * 0.2*(i + 1)):, :]))\n",
        "  y_validate_set = y[math.floor(len(y) * (0.2*i)) : math.floor(len(y) * 0.2* (i + 1))]\n",
        " \n",
        "  # x train plus one row of constant (to test the weight for the constant term)\n",
        "  temp_x = np.concatenate((np.ones((x_train_set.shape[0], 1)), x_train_set), axis = 1)\n",
        "  x_validate_set = np.concatenate((np.ones((x_validate_set.shape[0], 1)), x_validate_set), axis = 1)\n",
        "  gradient = np.zeros((dim, 1))\n",
        "\n",
        "  for t in range(iter_time):\n",
        "    loss = np.sqrt(np.sum(np.power(np.dot(temp_x, w) - y_train_set, 2))/471/12)#rmse\n",
        "    if(t%1000==0):\n",
        "        print(\"  \" + str(t) + \":\" + str(loss))\n",
        "    # gradient descent\n",
        "    gradient = 2 * np.dot(temp_x.transpose(), np.dot(temp_x, w) - y_train_set) #dim*1\n",
        "\n",
        "    # if (gradient){\n",
        "    #     break;\n",
        "    # }\n",
        "\n",
        "    #adagrad gradient optimization\n",
        "    adagrad += gradient ** 2\n",
        "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
        "  \n",
        "  final_w += w\n",
        "  print(\"for the \", i,\"th time, the parameter is roughly \", w[rand.randrange(1,18),0])\n",
        "  valid_loss = np.sqrt(np.sum(np.power(np.dot(x_validate_set, w) - y_validate_set, 2))/471/12)\n",
        "  print(\"the loss for the \", i,\"th time validate is \", valid_loss)\n",
        "  sum_loss += valid_loss\n",
        "\n",
        "print('average loss for the 5-fold validation is: ', sum_loss/k_fold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dBshXDetkfm",
        "colab_type": "text"
      },
      "source": [
        "## **TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZXdK7Aptegk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 9 * 18 + 1 # 9 * 18 features with 1 more constant\n",
        "learning_rate = 0.3\n",
        "# iter_time = 5000\n",
        "t = 0\n",
        "oldloss = 1\n",
        "newloss = 1\n",
        "adagrad = np.zeros([dim, 1])\n",
        "eps = 0.0000000001\n",
        "w = np.random.rand(dim ,1)\n",
        "normed_x1 = np.concatenate((np.ones([12 * 471, 1]), normed_x), axis = 1)\n",
        "\n",
        "while True:\n",
        "    oldloss = newloss\n",
        "    loss = np.sqrt(np.sum(np.power(np.dot(normed_x1, w) - y, 2))/471/12)#rmse\n",
        "    newloss = loss\n",
        "    if (abs((newloss - oldloss)/oldgloss) < 0.000008):\n",
        "      break;\n",
        "    if(t%1000==0):\n",
        "        print(str(t) + \":\" + str(loss))\n",
        "    gradient = 2 * np.dot(normed_x1.transpose(), np.dot(normed_x1, w) - y) #dim*1\n",
        "    adagrad += gradient ** 2\n",
        "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
        "    t = t + 1\n",
        "np.save('model_weights.npy', w)\n",
        "w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs284OejTilx",
        "colab_type": "text"
      },
      "source": [
        "## **FEATURE ENGINEERING FOR TESTING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jdgrni-TiIq",
        "colab_type": "code",
        "outputId": "d9bb3bc3-3660-4072-e219-63dd5df9e105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "! gdown --id '1JnF9biNzFqx5_9RKzCKPKgHPggtDHhue'\n",
        "test = pd.read_csv('./test.csv', encoding = 'big5')\n",
        "test = test.iloc[:, 2:]\n",
        "test[test == 'NR'] = 0\n",
        "test = test.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JnF9biNzFqx5_9RKzCKPKgHPggtDHhue\n",
            "To: /content/test.csv\n",
            "\r  0% 0.00/197k [00:00<?, ?B/s]\r100% 197k/197k [00:00<00:00, 56.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rk43DF45bWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# noticed that test data lacked the first row of data, which is AMB_TEMP, manually fill them up\n",
        "add = np.empty((1,9))\n",
        "add[0] = [21, 21, 20, 20, 19, 19, 19, 18, 17]\n",
        "test = np.concatenate((add,test), axis = 0)\n",
        "test_x = np.empty([240, 18*9], dtype = float)\n",
        "for i in range(240):\n",
        "    test_x[i, :] = test[18 * i: 18* (i + 1), :].reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK8FCa_4g11a",
        "colab_type": "text"
      },
      "source": [
        "## **NORMALIZATION FOR TESTING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ_laKAag0DJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(test_x)):\n",
        "    for j in range(len(test_x[0])):\n",
        "        if std_x[j] != 0:\n",
        "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
        "normed_test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGi6UiEUkYQn",
        "colab_type": "text"
      },
      "source": [
        "## **PREDICTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txt6tH7MkXwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_w = np.load('model_weights.npy')\n",
        "ans_y = np.dot(normed_test_x, test_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toxmjS1IkokP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(np.empty((240, 2)), index = np.arange(240) + 1, columns = ['id', 'value'])\n",
        "for i in range(240):\n",
        "  df.iloc[i, 0] = 'id_' + str(i)\n",
        "  df.iloc[i,1] = ans_y[i][0]\n",
        "df.to_csv('submission.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}